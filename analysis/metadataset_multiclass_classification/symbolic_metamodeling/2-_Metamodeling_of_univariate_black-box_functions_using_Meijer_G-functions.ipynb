{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Metamodeling of Univariate Functions using Meijer $G$-functions\n",
    "\n",
    "In this notebook, we carry out the first experiment (Section 5.1) in our paper *\"Demystifying Black-box Models with Symbolic Metamodels\"* submitted to **NeurIPS 2019** by *Ahmed M. Alaa and Mihaela van der Schaar*. In this experiment, we demonstrate the first use case of symbolic metamodeling using synthetic data, where we show how can we learn symbolic expressions for unobserved black-box functions for which we have only query access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we learn complex symbolic expressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off with four synthetic experiments with the aim of evaluating the richness of symbolic expressions discovered by our metamodeling algorithm. In each experiment, we apply our Meijer $G$-function-based symbolic metamodeling on a ground-truth univariate function $f(x)$ to fit a metamodel $g(x) \\approx f(x)$, and compare the resulting mathematical expression for $g(x)$ with that obtained by Symbolic regression [1-3], which we implement using the [**gplearn library** ](https://gplearn.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following four expressions for the underlying univariate functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Function** | **Notation** | **Expression**   |\n",
    "|------|------|------|\n",
    "|   Exponential function  | $f_1(x)$ | $e^{-3x}$ |\n",
    "|   Rational function  | $f_2(x)$| $\\frac{x}{(x+1)^2}$ |\n",
    "|   Sinusoid function  | $f_3(x)$| $\\sin(x)$ |\n",
    "|   Bessel function  | $f_4(x)$| $J_0\\left(10\\sqrt{x}\\right)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the functions $f_1(x)$, $f_2(x)$, $f_3(x)$ and $f_4(x)$ have very different functional forms and are of varying levels of complexity. To run the experiments, we first import the univariate functions above from the **benchmarks.univariate_functions** module in **pysymbolic** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysymbolic.benchmarks.univariate_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a list of the univariate functions $f_1(x)$, $f_2(x)$, $f_3(x)$ and $f_4(x)$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_functions  = [('Exponential function exp(-3x)', exponential_function), ('Rational function x/(x+1)^2', rational_function), \n",
    "                   ('Sinusoid function sin(x)', sinusoidal_function), ('Bessel function J_0(10*sqrt(x))', bessel_function)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the experimens, let us visualize the four functions in the range $x \\in [0,1]$ to see how different they are, and the extent to which their complexity vary from one function to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "x_points = np.linspace(0,1,100)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20,2.5))\n",
    "\n",
    "axs[0].plot(x_points, True_functions[0][1](x_points), linewidth=4)\n",
    "axs[0].set_title('$f_1(x)$')\n",
    "axs[1].plot(x_points, True_functions[1][1](x_points), linewidth=4)\n",
    "axs[1].set_title('$f_2(x)$')\n",
    "axs[2].plot(x_points, True_functions[2][1](x_points), linewidth=4)\n",
    "axs[2].set_title('$f_3(x)$')\n",
    "axs[3].plot(x_points, True_functions[3][1](x_points), linewidth=4)\n",
    "axs[3].set_title('$f_4(x)$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='$x$', ylabel='$f(x)$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Bessel function is the most complex. So will our symbolic metamodeling algorithm be able top recover the underlying mathematical expression describing these function and recognizing their varying levels of complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the experiment by first setting the number of evaluation points (npoints=100) that we will input to both the symbolic metamodeling and the symbolic regression models, and creating an empty list of learned symbolic expressions and $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoints             = 100\n",
    "xrange              = [0.01, 1]\n",
    "\n",
    "symbolic_metamodels = []\n",
    "symbolic_regssion   = []\n",
    "\n",
    "sym_metamodel_R2    = []\n",
    "sym_regression_R2   = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the experiments, we first import the **algorithms.symbolic_expressions** from **pysymbolic**. This module contains two functions **get_symbolic_model** and **symbolic_regressor**, which recovers univariate metamodels and symbolic regression models respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpmath import *\n",
    "from sympy import *\n",
    "\n",
    "from pysymbolic.algorithms.symbolic_expressions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the experiments by feeding in each function in **true_function** to both the functions **get_symbolic_model** and **symbolic_regressor**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for true_function in True_functions:\n",
    "    \n",
    "    print('Now working on the ' + true_function[0])\n",
    "    print('--------------------------------------------------------')\n",
    "    print('--------------------------------------------------------')\n",
    "    \n",
    "    symbolic_model, _mod_R2 = get_symbolic_model(true_function[1], npoints, xrange)\n",
    "    \n",
    "    symbolic_metamodels.append(symbolic_model)\n",
    "    sym_metamodel_R2.append(_mod_R2)\n",
    "    \n",
    "    symbolic_reg, _reg_R2   = symbolic_regressor(true_function[1], npoints, xrange)\n",
    "    \n",
    "    symbolic_regssion.append(symbolic_reg)\n",
    "    sym_regression_R2.append(_reg_R2) \n",
    "    \n",
    "    print('--------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check the symbolic expressions retrieved by both symbolic metamodeling and symbolic regression. In order to enable printing in LaTex format, we first invoke the \"init_print\" command of sympy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start with the first function $f_1(x) = e^{-3x}$, and see what the corresponding symbolic metamodel stroed in **symbolic_metamodels[0]**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_metamodels[0].expression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this is almost exactly equal to $e^{-3x}$! This means that the metamodeling algorithm was able to recover the true expression for $f_1(x)$ based on 100 evaluation samples only. To check the corresponding values of the poles and zeros recovered by the gradient descent algorithm used to optimize the metamodel, we can inspect the attributes of the **MeijerG** object **symbolic_metamodels[0]** as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_metamodels[0].a_p, symbolic_metamodels[0].b_q, symbolic_metamodels[0]._const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check the expression learned by symbolic regression (whcih is stored in **symbolic_regssion[0]**)... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_regssion[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the symbolic regression algorithm retreived an approximation of $f_1(x) = e^{-3x}$, but failed to capture the exponential functional form of $f_1(x)$. This is because the symbolic regression search algorithm starts with predefined forms (mostly polynomials), and hence is less flexible than our Meijer $G$-function parameterization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What if we want to restrict our metamodels to polynomials only?** In this case, we can use the *approximate_expression* method to recover a Taylor approximation of the learned symbolic expression as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "polynomial_metamodel_of_f1 = deepcopy(symbolic_metamodels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_metamodel_of_f1.approximation_order = 2\n",
    "\n",
    "polynomial_metamodel_of_f1.approx_expression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the second order Taylor approximation of our metamodel appears to be very closed to the symbolic regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the other functions? Let us check $f_2(x) = \\frac{x}{(x+1)^2}$ and see what the metamodel was for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_metamodels[1].expression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $f_2(x)$, the metamodeling algorithm nailed it! It exactly recovered the true symbolic expression. For the symbolic regression model for $f_2(x)$, we have the following expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_regssion[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the symbolic regression algorithm also did a good job in finding the true mathematical expression for $f_2(x)$, though it recovered a less accurate expression than that of the metamodel. Now let us examine the results third function $f_3(x) = \\sin(x)$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_metamodels[2].expression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_regssion[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, both algorithms came up with approximations of the sinusoid function in the range $[0,1]$. This is because in the range $[0,1]$ we see no full cycles of sinusoid, and hence it is indistiguishable from, say, a linear approximation. The confluent hypergeometric function $_2 F_1$ in the metamodel is very close to 0, and hence the metamodel can be though of as a linear approximation for the sinusoidal function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the most tricky of the four functions: $f_4(x) = J_0\\left(10\\sqrt{x}\\right)$. This one is diffcult because it already displays a lot of fluctuations in the range $[0,1]$, and has an unusual functional form. So what symbolic expressions did the two algorithms learn for $f_4(x)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_metamodels[3].expression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_regssion[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exciting result! The symbolic metamodel is very close to the ground truth: it corresponded to a Bessel function of the second kind $I_0(x)$ instead of a Bessel function of the first kind $J_0(x)$! Using the identity $J_0(ix) = I_0(x)$, we can see that our metamodel is in fact identical to the ground truth! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above \"qualitative\" comparisons show that symbolic metamodeling can recover richer and more complex expressions compared to symbolic regression. The quantitative comparison can be done by simply comparing the $R^2$ scores for the two algorithms on the four functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_metamodel_R2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_regression_R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to evaluate the numeric value of any metamodel for a given $x$, we can use the **evaluate** method of the **MeijerG** object. In the cell below, we evaluate all metamodels in the range $[0,1]$ and plot them along the true functions to see how accurate they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "x_points = np.linspace(0,1,100)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20,2.5))\n",
    "\n",
    "axs[0].plot(x_points, True_functions[0][1](x_points), linewidth=4, label='True function')\n",
    "axs[0].plot(x_points, symbolic_metamodels[0].evaluate(x_points), color='red', linewidth=3, linestyle='--', label='Metamodel')\n",
    "axs[0].set_title('$f_1(x)$')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(x_points, True_functions[1][1](x_points), linewidth=4, label='True function')\n",
    "axs[1].plot(x_points, symbolic_metamodels[1].evaluate(x_points), color='red', linewidth=3, linestyle='--', label='Metamodel')\n",
    "axs[1].set_title('$f_2(x)$')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(x_points, True_functions[2][1](x_points), linewidth=4, label='True function')\n",
    "axs[2].plot(x_points, symbolic_metamodels[2].evaluate(x_points), color='red', linewidth=3, linestyle='--', label='Metamodel')\n",
    "axs[2].set_title('$f_3(x)$')\n",
    "axs[2].legend()\n",
    "\n",
    "axs[3].plot(x_points, True_functions[3][1](x_points), linewidth=4, label='True function')\n",
    "axs[3].plot(x_points, symbolic_metamodels[3].evaluate(x_points), color='red', linewidth=3, linestyle='--', label='Metamodel')\n",
    "axs[3].set_title('$f_4(x)$')\n",
    "axs[3].legend()\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='$x$', ylabel='$f(x)$')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Patryk Orzechowski, William La Cava, and Jason H Moore. Where are we now?: a large benchmark study of recent symbolic regression methods. *In Proceedings of the Genetic and Evolutionary Computation Conference*, pages 1183–1190. ACM, 2018.\n",
    "\n",
    "[2] Telmo Menezes and Camille Roth. Symbolic regression of generative network models. *Scientific reports*, 4:6284, 2014.\n",
    "\n",
    "[3] Ekaterina J Vladislavleva, Guido F Smits, and Dick Den Hertog. Order of nonlinearity as a com344 plexity measure for models generated by symbolic regression via pareto genetic programming. *IEEE Transactions on Evolutionary Computation*, 13(2):333–349, 2009.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
